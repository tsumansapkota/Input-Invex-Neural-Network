{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random, os, pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn\n",
    "from classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.FashionMNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255.\n",
    "\n",
    "# train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting data to pytorch format\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_label = torch.LongTensor(train_label_)\n",
    "test_label = torch.LongTensor(test_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data():\n",
    "    global train_data, train_label\n",
    "    randidx = random.sample(range(len(train_label)), k=len(train_label))\n",
    "    train_data = train_data[randidx]\n",
    "    train_label = train_label[randidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_seeds = [147, 258, 369]\n",
    "# network_seeds = [369]\n",
    "network_seed = 369\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "actf = nn.LeakyReLU\n",
    "# actf = nn.ELU\n",
    "\n",
    "learning_rate = 0.005\n",
    "lambda_ = 2\n",
    "criterion = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "use_mixup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_OneClass_Balanced(data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, label, class_index):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.class_index = class_index\n",
    "        \n",
    "        mask = (label==class_index)\n",
    "        self.label = mask.type(torch.float32).reshape(-1,1)\n",
    "        self.class_data = torch.nonzero(mask).reshape(-1)\n",
    "        self.other_data = torch.nonzero(~mask).reshape(-1)\n",
    "        \n",
    "        random.seed(network_seed)\n",
    "        self._shuffle_data_()\n",
    "        self.count = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 2*len(self.class_data)\n",
    "    \n",
    "    def _shuffle_data_(self):\n",
    "#         randidx = np.random.permutation(len(self.other_data))\n",
    "        randidx = random.sample(range(len(self.other_data)), k=len(self.other_data))\n",
    "        self.other_data = self.other_data[randidx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.class_data):\n",
    "            idx = self.class_data[idx]\n",
    "            img, lbl = self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            idx = self.other_data[idx-len(self.class_data)]\n",
    "            img, lbl = self.data[idx], self.label[idx]\n",
    "            self.count += 1\n",
    "            if self.count >= len(self.class_data): \n",
    "                self._shuffle_data_()\n",
    "                self.count = 0\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_idx = 0\n",
    "# train_dataset = MNIST_OneClass_Balanced(train_data, train_label, class_idx)\n",
    "# test_dataset = MNIST_OneClass_Balanced(test_data, test_label, class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_all = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "# test_loader_all = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # img, lbl = train_dataset[11010]\n",
    "# img, lbl = test_dataset[10]\n",
    "# print(lbl)\n",
    "# plt.imshow(img.reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnivariateCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels:list, actf=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        assert len(channels)>1\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            la = nn.Conv2d(channels[i], channels[i+1], kernel_size=(5,5), stride=2, padding=1)\n",
    "            layers.append(la)\n",
    "            layers.append(actf())\n",
    "        layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.fc = nn.Sequential(nn.Linear(channels[-1], 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,1, 28, 28)\n",
    "        x = self.features(x)\n",
    "        s = x.shape\n",
    "        return self.fc(x.reshape(s[0], s[1]))    \n",
    "    \n",
    "\n",
    "class ConvexCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels:list, actf=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        assert len(channels)>1\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            la = nn.Conv2d(channels[i], channels[i+1], kernel_size=(5,5), stride=2, padding=1)\n",
    "            layers.append(la)\n",
    "            if i>0:\n",
    "                layers[-1].weight.data *= 0.1\n",
    "            layers.append(actf())\n",
    "        layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.fc = nn.Sequential(nn.Linear(channels[-1], 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,1,28, 28)\n",
    "        for i in range(2, len(self.features)-1, 2):\n",
    "            self.features[i].weight.data.abs_()\n",
    "        for i in range(0, len(self.fc), 2):\n",
    "            self.fc[i].weight.data.abs_()\n",
    "            \n",
    "        x = self.features(x)\n",
    "        s = x.shape\n",
    "        return self.fc(x.reshape(s[0], s[1]))    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = UnivariateCNN([1, 16, 32, 64])\n",
    "# cnn = ConvexCNN([1, 32, 32])\n",
    "# cnn(torch.randn(2,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = nn.Sequential(nn.Conv2d(1, 1, kernel_size=5, padding=2, stride=2),\n",
    "#              nn.Conv2d(1,1, kernel_size=5, padding=2, stride=2))\n",
    "# # b = nn.Conv2d(1, 1, kernel_size=5, padding=2, stride=4)\n",
    "# b = nn.Conv2d(1, 1, kernel_size=5, padding=2, stride=2, dilation=2)\n",
    "# x = torch.randn(2,1,32,32)\n",
    "\n",
    "# a(x).shape, b(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = nn.Sequential(nn.Conv2d(1, 1, kernel_size=5, padding=2, stride=2),\n",
    "#                  nn.Conv2d(1,1, kernel_size=5, padding=2, stride=2),\n",
    "#                  nn.Conv2d(1,1, kernel_size=5, padding=2, stride=2))\n",
    "# b = nn.Conv2d(1, 1, kernel_size=5, padding=2, stride=4, dilation=4)\n",
    "# x = torch.randn(2,1,28,28)\n",
    "# # nn.Conv2d()\n",
    "\n",
    "# a(x).shape, b(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_per_class = []\n",
    "net_list = []\n",
    "for class_idx in range(10):\n",
    "    print(class_idx)\n",
    "    train_dataset = MNIST_OneClass_Balanced(train_data, train_label, class_idx)\n",
    "    test_dataset = MNIST_OneClass_Balanced(test_data, test_label, class_idx)\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(network_seed)\n",
    "    Net = ConvexCNN([1, 32, 32], actf)\n",
    "    optimizer = torch.optim.Adam(Net.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    index = 0\n",
    "#     EPOCHS = 5\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for xx, yy in train_loader:\n",
    "            index += 1\n",
    "            if use_mixup:\n",
    "                rand_indx = np.random.permutation(len(xx))\n",
    "                rand_lambda = 1-torch.rand(len(xx), 1)*0.1\n",
    "                x_mix = rand_lambda*xx+(1-rand_lambda)*xx[rand_indx]\n",
    "                y_mix = rand_lambda*yy+(1-rand_lambda)*yy[rand_indx]\n",
    "            else:\n",
    "                x_mix = xx\n",
    "                y_mix = yy\n",
    "\n",
    "            yout = sigmoid(Net(x_mix))    \n",
    "            loss = criterion(yout, y_mix)\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = (yout.data.cpu().numpy() > 0.5).astype(float)\n",
    "            correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "#             if index%200 == 0:\n",
    "        train_accs.append(float(train_acc)/train_count*100)\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "\n",
    "        print(f'Epoch: {epoch}:{index},  Loss:{float(loss)}')\n",
    "        test_count = 0\n",
    "        test_acc = 0\n",
    "        for xx, yy in test_loader:\n",
    "            with torch.no_grad():\n",
    "                yout = sigmoid(Net(xx))\n",
    "            outputs = (yout.data.cpu().numpy() > 0.5).astype(float)\n",
    "            correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "            test_acc += correct\n",
    "            test_count += len(xx)\n",
    "        test_accs.append(float(test_acc)/test_count*100)\n",
    "        print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "        print()\n",
    "                \n",
    "    ### after each class index is finished training\n",
    "    stat_per_class.append(\n",
    "    f'Class: {class_idx} -> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}'\n",
    "    )\n",
    "    print(stat_per_class[-1], '\\n')\n",
    "    net_list.append(Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the classification error from classification per class network.\n",
    "acc_test = 0\n",
    "count_test = 0\n",
    "with torch.no_grad():\n",
    "    for index in range(len(test_label) // batch_size):\n",
    "        xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "        yy = test_label[index * batch_size:(index + 1) * batch_size]\n",
    "        yout = []\n",
    "        for net in net_list:\n",
    "            yout.append(sigmoid(net(xx)).reshape(-1))\n",
    "        yout = torch.stack(yout, dim=1).argmax(dim=1)\n",
    "        acc = (yout == yy).type(torch.float32).sum()\n",
    "        count_test += len(xx)\n",
    "        acc_test += acc\n",
    "        \n",
    "for s in stat_per_class:\n",
    "    print(s)\n",
    "print(f\"Total Accuracy (Argmax) is : {float(acc_test/count_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## find the classification acc for confident examples \n",
    "# acc_test = 0\n",
    "# count_test = 0\n",
    "# certain_percent = 0\n",
    "# with torch.no_grad():\n",
    "#     for index in range(len(test_label) // batch_size):\n",
    "#         xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "#         yy = test_label[index * batch_size:(index + 1) * batch_size]\n",
    "#         yout = []\n",
    "#         for net in net_list:\n",
    "#             yout.append(sigmoid(net(xx)).reshape(-1))\n",
    "#         yout_ = torch.stack(yout, dim=1)\n",
    "# #         print(yout_)\n",
    "# #         print(yy)\n",
    "# #         break\n",
    "#         yout_, yout = yout_.max(dim=1)\n",
    "#         mask = yout_<0.5\n",
    "#         certain_percent += (mask).type(torch.float32).sum()\n",
    "#         acc = (yout == yy).type(torch.float32)[~mask].sum()\n",
    "#         count_test += len(xx)\n",
    "#         acc_test += acc\n",
    "        \n",
    "# print(f\"UNConfident {float(certain_percent/count_test)}% (Argmax) is : {float(acc_test/(count_test-certain_percent))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_per_class = []\n",
    "net_list = []\n",
    "for class_idx in range(10):\n",
    "    print(class_idx)\n",
    "    train_dataset = MNIST_OneClass_Balanced(train_data, train_label, class_idx)\n",
    "    test_dataset = MNIST_OneClass_Balanced(test_data, test_label, class_idx)\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(network_seed)\n",
    "    Net = UnivariateCNN([1, 32, 32], actf)\n",
    "    optimizer = torch.optim.Adam(Net.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    index = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for xx, yy in train_loader:\n",
    "            index += 1\n",
    "\n",
    "            if use_mixup:\n",
    "                rand_indx = np.random.permutation(len(xx))\n",
    "                rand_lambda = 1-torch.rand(len(xx), 1)*0.1\n",
    "                x_mix = rand_lambda*xx+(1-rand_lambda)*xx[rand_indx]\n",
    "                y_mix = rand_lambda*yy+(1-rand_lambda)*yy[rand_indx]\n",
    "            else:\n",
    "                x_mix = xx\n",
    "                y_mix = yy\n",
    "\n",
    "            yout = sigmoid(Net(x_mix))    \n",
    "            loss = criterion(yout, y_mix)\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = (yout.data.cpu().numpy() > 0.5).astype(float)\n",
    "            correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "#             if index%200 == 0:\n",
    "        train_accs.append(float(train_acc)/train_count*100)\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "\n",
    "        print(f'Epoch: {epoch}:{index},  Loss:{float(loss)}')\n",
    "        test_count = 0\n",
    "        test_acc = 0\n",
    "        for xx, yy in test_loader:\n",
    "            with torch.no_grad():\n",
    "                yout = sigmoid(Net(xx))\n",
    "            outputs = (yout.data.cpu().numpy() > 0.5).astype(float)\n",
    "            correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "            test_acc += correct\n",
    "            test_count += len(xx)\n",
    "        test_accs.append(float(test_acc)/test_count*100)\n",
    "        print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "        print()\n",
    "                \n",
    "    ### after each class index is finished training\n",
    "    stat_per_class.append(\n",
    "    f'Class: {class_idx} -> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}'\n",
    "    )\n",
    "    print(stat_per_class[-1], '\\n')\n",
    "    net_list.append(Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## find the classification error from classification per class network.\n",
    "acc_test = 0\n",
    "count_test = 0\n",
    "with torch.no_grad():\n",
    "    for index in range(len(test_label) // batch_size):\n",
    "        xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "        yy = test_label[index * batch_size:(index + 1) * batch_size]\n",
    "        yout = []\n",
    "        for net in net_list:\n",
    "            yout.append(sigmoid(net(xx)).reshape(-1))\n",
    "        yout = torch.stack(yout, dim=1).argmax(dim=1)\n",
    "        acc = (yout == yy).type(torch.float32).sum()\n",
    "        count_test += len(xx)\n",
    "        acc_test += acc\n",
    "        \n",
    "for s in stat_per_class:\n",
    "    print(s)\n",
    "print(f\"Total Accuracy (Argmax) is : {float(acc_test/count_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## find the classification acc for confident examples \n",
    "# acc_test = 0\n",
    "# count_test = 0\n",
    "# certain_percent = 0\n",
    "# with torch.no_grad():\n",
    "#     for index in range(len(test_label) // batch_size):\n",
    "#         xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "#         yy = test_label[index * batch_size:(index + 1) * batch_size]\n",
    "#         yout = []\n",
    "#         for net in net_list:\n",
    "#             yout.append(sigmoid(net(xx)).reshape(-1))\n",
    "#         yout_ = torch.stack(yout, dim=1)\n",
    "# #         print(yout_)\n",
    "# #         print(yy)\n",
    "# #         break\n",
    "#         yout_, yout = yout_.max(dim=1)\n",
    "#         mask = yout_<0.5\n",
    "#         certain_percent += (mask).type(torch.float32).sum()\n",
    "#         acc = (yout == yy).type(torch.float32)[~mask].sum()\n",
    "#         count_test += len(xx)\n",
    "#         acc_test += acc\n",
    "        \n",
    "# print(f\"UNConfident {float(certain_percent/count_test)}% (Argmax) is : {float(acc_test/(count_test-certain_percent))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mixup = False\n",
    "use_check = False\n",
    "check_every = 2\n",
    "check_size = 100\n",
    "\n",
    "m_,s_ = 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0:240,  Loss:0.23692530393600464, MinVal:0.33153805136680603, gp: 1.9162254716320604e-07\n",
      "Train Acc:76.11%, Test Acc:90.30%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.2016894519329071, MinVal:0.1023336723446846, gp: 0.0006862402078695595\n",
      "Train Acc:91.73%, Test Acc:93.35%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.12320880591869354, MinVal:0.3461264967918396, gp: 1.46266700085107e-07\n",
      "Train Acc:93.03%, Test Acc:93.85%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.19235289096832275, MinVal:0.20166996121406555, gp: 1.8917211491498165e-05\n",
      "Train Acc:93.65%, Test Acc:93.80%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.1601240038871765, MinVal:0.44365864992141724, gp: 1.6671741764895387e-09\n",
      "Train Acc:94.06%, Test Acc:93.70%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.08797988295555115, MinVal:0.16968077421188354, gp: 6.147215026430786e-05\n",
      "Train Acc:94.34%, Test Acc:94.70%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.20766393840312958, MinVal:0.33422863483428955, gp: 1.0775615777447456e-07\n",
      "Train Acc:94.58%, Test Acc:94.75%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.10020942986011505, MinVal:0.4538019597530365, gp: 9.159416558546241e-10\n",
      "Train Acc:94.77%, Test Acc:94.70%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.10784950107336044, MinVal:0.9396374225616455, gp: 5.546095873886872e-18\n",
      "Train Acc:95.01%, Test Acc:94.20%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.31423062086105347, MinVal:0.9223265051841736, gp: 8.059147089029631e-18\n",
      "Train Acc:95.11%, Test Acc:94.50%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.20422476530075073, MinVal:0.4200493097305298, gp: 3.438473283878807e-09\n",
      "Train Acc:94.98%, Test Acc:94.80%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.16888882219791412, MinVal:0.4095045030117035, gp: 5.356077359408573e-09\n",
      "Train Acc:95.18%, Test Acc:94.45%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.05453646555542946, MinVal:1.1203361749649048, gp: 3.2670978574493257e-21\n",
      "Train Acc:95.45%, Test Acc:94.70%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.15095281600952148, MinVal:0.7083792090415955, gp: 3.375197934725807e-14\n",
      "Train Acc:95.35%, Test Acc:94.95%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.0322432816028595, MinVal:0.7278059124946594, gp: 1.5517858740067385e-14\n",
      "Train Acc:95.89%, Test Acc:94.10%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.04943984001874924, MinVal:1.1786611080169678, gp: 2.341681974090936e-22\n",
      "Train Acc:95.66%, Test Acc:95.25%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.16229093074798584, MinVal:1.1071970462799072, gp: 3.98725208278211e-21\n",
      "Train Acc:95.77%, Test Acc:94.65%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.13871823251247406, MinVal:1.0375868082046509, gp: 6.447183988496959e-20\n",
      "Train Acc:95.99%, Test Acc:94.75%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.12126286327838898, MinVal:0.6455005407333374, gp: 4.174336928636918e-13\n",
      "Train Acc:96.16%, Test Acc:95.00%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.05837708339095116, MinVal:0.6473943591117859, gp: 3.869804867176474e-13\n",
      "Train Acc:96.31%, Test Acc:94.80%\n",
      "\n",
      "Class: 0 -> Train Acc 96.30833333333332 ; Test Acc 95.25 \n",
      "\n",
      "1\n",
      "Epoch: 0:240,  Loss:0.027205154299736023, MinVal:0.3350919783115387, gp: 1.1568161539798894e-07\n",
      "Train Acc:90.35%, Test Acc:97.20%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.028136033564805984, MinVal:0.6272876262664795, gp: 1.4031254456925435e-12\n",
      "Train Acc:96.99%, Test Acc:97.20%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.016264377161860466, MinVal:0.1585519015789032, gp: 9.139980829786509e-05\n",
      "Train Acc:97.52%, Test Acc:97.60%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.041699256747961044, MinVal:0.6294505000114441, gp: 1.0570913719018526e-12\n",
      "Train Acc:97.93%, Test Acc:97.70%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.07566924393177032, MinVal:0.38559168577194214, gp: 2.2577587799332832e-08\n",
      "Train Acc:98.08%, Test Acc:97.70%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.009676799178123474, MinVal:0.6764736771583557, gp: 2.1905833267124586e-13\n",
      "Train Acc:98.28%, Test Acc:98.10%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.05010117590427399, MinVal:-0.002597425365820527, gp: 0.0059015084989368916\n",
      "Train Acc:98.52%, Test Acc:98.30%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.006316874176263809, MinVal:0.731340229511261, gp: 2.5273669130286038e-14\n",
      "Train Acc:98.67%, Test Acc:98.00%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.0275197122246027, MinVal:0.8008885383605957, gp: 8.439127955139707e-16\n",
      "Train Acc:98.81%, Test Acc:98.40%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.021678684279322624, MinVal:0.7322536110877991, gp: 1.3052242392366104e-14\n",
      "Train Acc:98.98%, Test Acc:98.15%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.006766197271645069, MinVal:0.7849299311637878, gp: 1.579822855143269e-15\n",
      "Train Acc:98.86%, Test Acc:98.45%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.0014127164613455534, MinVal:0.7448130249977112, gp: 7.860270932942039e-15\n",
      "Train Acc:99.00%, Test Acc:98.55%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.052280645817518234, MinVal:0.6892268061637878, gp: 7.261198071853783e-14\n",
      "Train Acc:99.08%, Test Acc:98.50%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.006464166101068258, MinVal:0.963278591632843, gp: 1.4754851754002648e-18\n",
      "Train Acc:99.21%, Test Acc:98.65%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.03264230117201805, MinVal:1.5160554647445679, gp: 3.145728508020829e-28\n",
      "Train Acc:99.15%, Test Acc:98.75%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.05539052188396454, MinVal:1.5780162811279297, gp: 2.797583568651917e-29\n",
      "Train Acc:99.27%, Test Acc:98.85%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.0034645043779164553, MinVal:1.3140068054199219, gp: 1.1895287824014871e-24\n",
      "Train Acc:99.35%, Test Acc:99.10%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.0016072308644652367, MinVal:1.4721354246139526, gp: 1.841635649988301e-27\n",
      "Train Acc:99.42%, Test Acc:98.40%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.0012501636520028114, MinVal:2.064058542251587, gp: 1.2365666373255089e-37\n",
      "Train Acc:99.46%, Test Acc:99.00%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.04987064749002457, MinVal:1.746196985244751, gp: 3.221010862001727e-32\n",
      "Train Acc:99.43%, Test Acc:98.85%\n",
      "\n",
      "Class: 1 -> Train Acc 99.45833333333334 ; Test Acc 99.1 \n",
      "\n",
      "2\n",
      "Epoch: 0:240,  Loss:0.29203301668167114, MinVal:0.4160747230052948, gp: 5.793937329912069e-09\n",
      "Train Acc:71.95%, Test Acc:86.75%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.3379763662815094, MinVal:0.2057543396949768, gp: 3.5598091926658526e-05\n",
      "Train Acc:87.50%, Test Acc:87.65%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.2952260375022888, MinVal:0.43455740809440613, gp: 2.0148755996984846e-09\n",
      "Train Acc:88.34%, Test Acc:88.10%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.3044518828392029, MinVal:0.2860875427722931, gp: 8.119773156067822e-07\n",
      "Train Acc:89.22%, Test Acc:89.80%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.5735405683517456, MinVal:0.2287088930606842, gp: 6.743929588992614e-06\n",
      "Train Acc:90.33%, Test Acc:90.25%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.15810014307498932, MinVal:0.3639524579048157, gp: 3.625113009775305e-08\n",
      "Train Acc:90.80%, Test Acc:90.10%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.21319109201431274, MinVal:0.4265991449356079, gp: 2.658513187370204e-09\n",
      "Train Acc:91.27%, Test Acc:91.90%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.29647424817085266, MinVal:0.2665562927722931, gp: 1.6742517345846863e-06\n",
      "Train Acc:91.62%, Test Acc:91.20%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.21733665466308594, MinVal:0.47871509194374084, gp: 3.327121911311792e-10\n",
      "Train Acc:91.83%, Test Acc:92.00%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.31384164094924927, MinVal:0.5836604237556458, gp: 4.95274090836495e-12\n",
      "Train Acc:92.33%, Test Acc:91.85%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.15043067932128906, MinVal:0.6965510249137878, gp: 5.416974919840445e-14\n",
      "Train Acc:92.77%, Test Acc:92.60%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.22995136678218842, MinVal:0.8398235440254211, gp: 3.2401622799723675e-16\n",
      "Train Acc:92.95%, Test Acc:91.95%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.12187835574150085, MinVal:1.1834508180618286, gp: 1.9197681124135093e-22\n",
      "Train Acc:93.29%, Test Acc:92.40%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.20255649089813232, MinVal:0.916941225528717, gp: 8.038747160762849e-18\n",
      "Train Acc:93.63%, Test Acc:92.50%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.18029722571372986, MinVal:1.0073540210723877, gp: 2.5474425415793585e-19\n",
      "Train Acc:93.83%, Test Acc:93.50%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.21047063171863556, MinVal:0.7661052942276001, gp: 3.55855058284003e-15\n",
      "Train Acc:94.06%, Test Acc:93.35%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.07944580167531967, MinVal:1.2531096935272217, gp: 2.2809523377178377e-23\n",
      "Train Acc:94.23%, Test Acc:92.65%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.2451079785823822, MinVal:0.4944985806941986, gp: 1.7522859552254744e-10\n",
      "Train Acc:94.75%, Test Acc:93.75%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.07626891136169434, MinVal:0.5551475286483765, gp: 1.5493117205833684e-11\n",
      "Train Acc:94.92%, Test Acc:93.65%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.12526820600032806, MinVal:0.8505030870437622, gp: 1.1464301097000317e-16\n",
      "Train Acc:95.14%, Test Acc:93.10%\n",
      "\n",
      "Class: 2 -> Train Acc 95.14166666666667 ; Test Acc 93.75 \n",
      "\n",
      "3\n",
      "Epoch: 0:240,  Loss:0.2995576858520508, MinVal:0.45038914680480957, gp: 1.6629607690887838e-09\n",
      "Train Acc:84.48%, Test Acc:91.75%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.07989824563264847, MinVal:0.3315035104751587, gp: 1.891674514808983e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:93.08%, Test Acc:92.00%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.3062463700771332, MinVal:0.5335273146629333, gp: 4.3394669801966046e-11\n",
      "Train Acc:94.04%, Test Acc:91.95%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.08795896172523499, MinVal:0.28868794441223145, gp: 6.4488244788663e-07\n",
      "Train Acc:94.54%, Test Acc:94.00%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.0688890889286995, MinVal:0.303994357585907, gp: 3.5728683656088833e-07\n",
      "Train Acc:95.23%, Test Acc:94.70%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.18839824199676514, MinVal:0.3920791745185852, gp: 1.3092843254014497e-08\n",
      "Train Acc:95.38%, Test Acc:94.65%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.2698238492012024, MinVal:0.287703275680542, gp: 8.306432732752e-07\n",
      "Train Acc:95.66%, Test Acc:95.10%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.08608528971672058, MinVal:0.553786039352417, gp: 1.918309955328823e-11\n",
      "Train Acc:95.93%, Test Acc:95.05%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.09584604948759079, MinVal:0.5383269786834717, gp: 3.15392191529984e-11\n",
      "Train Acc:95.93%, Test Acc:94.75%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.054753612726926804, MinVal:0.35290229320526123, gp: 5.022210558536244e-08\n",
      "Train Acc:96.05%, Test Acc:95.10%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.039562124758958817, MinVal:0.6280290484428406, gp: 8.714233761669687e-13\n",
      "Train Acc:96.29%, Test Acc:94.70%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.23005640506744385, MinVal:0.6625424027442932, gp: 2.1115350949934414e-13\n",
      "Train Acc:96.30%, Test Acc:95.60%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.10106100142002106, MinVal:0.6299208998680115, gp: 7.786165375639476e-13\n",
      "Train Acc:96.57%, Test Acc:94.25%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.13938365876674652, MinVal:0.4730185568332672, gp: 4.1367978487194534e-10\n",
      "Train Acc:96.72%, Test Acc:94.70%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.07262404263019562, MinVal:1.0444362163543701, gp: 4.902002526312234e-20\n",
      "Train Acc:96.83%, Test Acc:95.25%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.023021796718239784, MinVal:0.3531675934791565, gp: 4.9681979419347044e-08\n",
      "Train Acc:97.10%, Test Acc:95.60%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.03495696932077408, MinVal:0.761414110660553, gp: 5.952079768048761e-15\n",
      "Train Acc:96.97%, Test Acc:95.55%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.04774639755487442, MinVal:0.6946753263473511, gp: 5.840223053429905e-14\n",
      "Train Acc:97.10%, Test Acc:94.65%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.04885862022638321, MinVal:0.8498581051826477, gp: 1.1784520579220825e-16\n",
      "Train Acc:97.37%, Test Acc:94.75%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.08599992096424103, MinVal:0.9230417609214783, gp: 7.905851496369467e-18\n",
      "Train Acc:97.31%, Test Acc:95.20%\n",
      "\n",
      "Class: 3 -> Train Acc 97.36666666666667 ; Test Acc 95.6 \n",
      "\n",
      "4\n",
      "Epoch: 0:240,  Loss:0.39673373103141785, MinVal:0.5611950159072876, gp: 1.5045624468523755e-11\n",
      "Train Acc:69.20%, Test Acc:83.95%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.2635818123817444, MinVal:0.46548575162887573, gp: 6.978917999056478e-10\n",
      "Train Acc:87.52%, Test Acc:89.95%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.30332517623901367, MinVal:0.23958761990070343, gp: 7.913338777143508e-06\n",
      "Train Acc:90.62%, Test Acc:90.95%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.24759244918823242, MinVal:0.3899662494659424, gp: 2.1036928643525243e-08\n",
      "Train Acc:91.61%, Test Acc:91.35%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.2947309911251068, MinVal:0.5244590044021606, gp: 8.536735207620438e-11\n",
      "Train Acc:92.22%, Test Acc:92.20%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.1347656548023224, MinVal:0.5046191215515137, gp: 1.965172607976129e-10\n",
      "Train Acc:92.74%, Test Acc:92.45%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.18039517104625702, MinVal:0.46038100123405457, gp: 1.5083636561996627e-09\n",
      "Train Acc:93.02%, Test Acc:92.10%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.21611806750297546, MinVal:0.487729012966156, gp: 2.863736747737988e-10\n",
      "Train Acc:93.18%, Test Acc:92.35%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.19855330884456635, MinVal:0.3742571771144867, gp: 2.1548991924191796e-08\n",
      "Train Acc:93.41%, Test Acc:92.20%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.29863786697387695, MinVal:0.5051020383834839, gp: 1.1502877123037436e-10\n",
      "Train Acc:93.63%, Test Acc:92.30%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.138006791472435, MinVal:0.263840913772583, gp: 1.726351911202073e-06\n",
      "Train Acc:93.85%, Test Acc:92.20%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.21070337295532227, MinVal:0.5054162740707397, gp: 1.201326399469238e-10\n",
      "Train Acc:93.97%, Test Acc:92.10%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.155625119805336, MinVal:0.2893519699573517, gp: 6.368242111420841e-07\n",
      "Train Acc:94.03%, Test Acc:93.40%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.0709744542837143, MinVal:0.5010506510734558, gp: 1.5736757730255846e-10\n",
      "Train Acc:94.30%, Test Acc:92.65%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.20022381842136383, MinVal:0.35840973258018494, gp: 4.066597725227439e-08\n",
      "Train Acc:94.32%, Test Acc:92.65%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.30573710799217224, MinVal:0.4839889407157898, gp: 2.669329202120707e-10\n",
      "Train Acc:94.55%, Test Acc:93.10%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.09442352503538132, MinVal:0.6997770667076111, gp: 4.765212543674825e-14\n",
      "Train Acc:94.64%, Test Acc:93.20%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.22309373319149017, MinVal:0.9357617497444153, gp: 3.9298618847552235e-18\n",
      "Train Acc:94.83%, Test Acc:92.95%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.1904255896806717, MinVal:0.5621633529663086, gp: 1.3239107726770172e-11\n",
      "Train Acc:95.15%, Test Acc:93.65%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.1596834659576416, MinVal:1.073333740234375, gp: 2.773360171775281e-20\n",
      "Train Acc:95.31%, Test Acc:93.45%\n",
      "\n",
      "Class: 4 -> Train Acc 95.30833333333332 ; Test Acc 93.65 \n",
      "\n",
      "5\n",
      "Epoch: 0:240,  Loss:0.2120373547077179, MinVal:0.24547192454338074, gp: 3.6644082683778834e-06\n",
      "Train Acc:88.20%, Test Acc:93.05%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.04325859993696213, MinVal:0.6237504482269287, gp: 1.0171866604222202e-12\n",
      "Train Acc:94.23%, Test Acc:96.15%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.12226098030805588, MinVal:0.8271545171737671, gp: 5.075228719952267e-16\n",
      "Train Acc:95.23%, Test Acc:96.75%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.0391857773065567, MinVal:0.47004222869873047, gp: 5.354893195530508e-10\n",
      "Train Acc:96.73%, Test Acc:98.10%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.09418307989835739, MinVal:0.7291824221611023, gp: 1.957724550505878e-14\n",
      "Train Acc:96.85%, Test Acc:96.75%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.07605592906475067, MinVal:0.6146583557128906, gp: 2.679109104702926e-12\n",
      "Train Acc:97.38%, Test Acc:98.50%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.009966031648218632, MinVal:0.9109563827514648, gp: 1.2888658466213347e-17\n",
      "Train Acc:97.64%, Test Acc:98.55%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.020899713039398193, MinVal:1.2330913543701172, gp: 4.799091168264115e-23\n",
      "Train Acc:97.97%, Test Acc:98.45%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.052550967782735825, MinVal:1.2242136001586914, gp: 4.318978115116486e-23\n",
      "Train Acc:98.22%, Test Acc:98.55%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.013082814402878284, MinVal:1.310802936553955, gp: 1.1567124645671325e-24\n",
      "Train Acc:98.49%, Test Acc:98.75%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.020113658159971237, MinVal:1.214430570602417, gp: 5.46148283979318e-23\n",
      "Train Acc:98.42%, Test Acc:98.75%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.002125870669260621, MinVal:1.4182710647583008, gp: 1.587836948584341e-26\n",
      "Train Acc:98.55%, Test Acc:98.90%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.008042228408157825, MinVal:1.565758466720581, gp: 4.308580633390983e-29\n",
      "Train Acc:98.60%, Test Acc:98.90%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.07058960199356079, MinVal:1.5013377666473389, gp: 5.665397376432129e-28\n",
      "Train Acc:98.82%, Test Acc:99.15%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.0009973742999136448, MinVal:1.519303321838379, gp: 3.457376713831738e-28\n",
      "Train Acc:98.59%, Test Acc:99.05%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.005491077434271574, MinVal:1.5149762630462646, gp: 5.443201875783202e-28\n",
      "Train Acc:99.03%, Test Acc:98.30%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.0180310420691967, MinVal:0.6141139268875122, gp: 1.4649467619878842e-12\n",
      "Train Acc:99.01%, Test Acc:99.05%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.024732718244194984, MinVal:1.7828251123428345, gp: 9.778761915021956e-33\n",
      "Train Acc:99.11%, Test Acc:98.90%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.012314260937273502, MinVal:2.536994457244873, gp: 0.0\n",
      "Train Acc:99.26%, Test Acc:98.60%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.024243582040071487, MinVal:1.509237289428711, gp: 4.130007506167737e-28\n",
      "Train Acc:99.23%, Test Acc:99.00%\n",
      "\n",
      "Class: 5 -> Train Acc 99.25833333333334 ; Test Acc 99.15 \n",
      "\n",
      "6\n",
      "Epoch: 0:240,  Loss:0.39360684156417847, MinVal:0.185472771525383, gp: 6.559541361639276e-05\n",
      "Train Acc:68.65%, Test Acc:78.90%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.4267905056476593, MinVal:0.33946216106414795, gp: 9.040990534003868e-08\n",
      "Train Acc:80.73%, Test Acc:80.90%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.4111057221889496, MinVal:0.32750245928764343, gp: 1.8671954649107647e-07\n",
      "Train Acc:82.02%, Test Acc:81.65%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.4754849374294281, MinVal:0.5753811001777649, gp: 7.1728395792391986e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:83.61%, Test Acc:82.85%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.20059801638126373, MinVal:0.526780366897583, gp: 5.006182274280846e-11\n",
      "Train Acc:85.64%, Test Acc:85.45%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.17083610594272614, MinVal:0.7516535520553589, gp: 6.0923657882894916e-15\n",
      "Train Acc:86.79%, Test Acc:86.25%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.2014707773923874, MinVal:0.7350465059280396, gp: 2.0161813435145073e-14\n",
      "Train Acc:87.76%, Test Acc:87.10%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.41311588883399963, MinVal:0.48853644728660583, gp: 2.4511534468807383e-10\n",
      "Train Acc:88.19%, Test Acc:86.95%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.23671171069145203, MinVal:0.6887109279632568, gp: 1.503332370698593e-13\n",
      "Train Acc:88.46%, Test Acc:86.55%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.2706350088119507, MinVal:0.4995724856853485, gp: 1.430468660190698e-10\n",
      "Train Acc:89.03%, Test Acc:86.95%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.20371194183826447, MinVal:0.6616938710212708, gp: 2.2147110263336794e-13\n",
      "Train Acc:89.69%, Test Acc:87.40%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.31545037031173706, MinVal:0.6306213736534119, gp: 7.643089720149598e-13\n",
      "Train Acc:89.84%, Test Acc:86.80%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.35575494170188904, MinVal:0.7096537947654724, gp: 3.207283139701651e-14\n",
      "Train Acc:90.35%, Test Acc:87.90%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.23556332290172577, MinVal:0.5932353734970093, gp: 3.3944136564045824e-12\n",
      "Train Acc:90.31%, Test Acc:83.80%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.23225364089012146, MinVal:0.8699037432670593, gp: 5.276231425045836e-17\n",
      "Train Acc:90.68%, Test Acc:88.00%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.19713614881038666, MinVal:0.49789270758628845, gp: 2.2087418005689585e-10\n",
      "Train Acc:91.22%, Test Acc:87.35%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.17404650151729584, MinVal:0.8104095458984375, gp: 5.737516369872116e-16\n",
      "Train Acc:91.31%, Test Acc:87.40%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.20564664900302887, MinVal:0.6720898747444153, gp: 1.4486442644415232e-13\n",
      "Train Acc:91.33%, Test Acc:87.50%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.3693429231643677, MinVal:0.9627939462661743, gp: 2.041310732189227e-18\n",
      "Train Acc:92.03%, Test Acc:84.75%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.21295955777168274, MinVal:0.9022895097732544, gp: 1.4444894563974687e-17\n",
      "Train Acc:92.63%, Test Acc:87.95%\n",
      "\n",
      "Class: 6 -> Train Acc 92.63333333333334 ; Test Acc 88.0 \n",
      "\n",
      "7\n",
      "Epoch: 0:240,  Loss:0.1198066845536232, MinVal:0.17172351479530334, gp: 0.00012039086141157895\n",
      "Train Acc:89.73%, Test Acc:94.25%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.1622682809829712, MinVal:0.3506941497325897, gp: 7.181234451536511e-08\n",
      "Train Acc:94.84%, Test Acc:94.85%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.052934352308511734, MinVal:0.3890473246574402, gp: 1.1871204463886897e-08\n",
      "Train Acc:95.75%, Test Acc:97.10%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.10570517927408218, MinVal:0.3356362283229828, gp: 9.990962013262106e-08\n",
      "Train Acc:96.54%, Test Acc:95.40%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.010626185685396194, MinVal:0.40084847807884216, gp: 7.421087300230056e-09\n",
      "Train Acc:96.78%, Test Acc:96.40%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.07179531455039978, MinVal:0.7162721157073975, gp: 2.4645768688684108e-14\n",
      "Train Acc:96.91%, Test Acc:97.85%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.012598713859915733, MinVal:1.119291067123413, gp: 4.3199519607485375e-21\n",
      "Train Acc:97.54%, Test Acc:97.60%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.04672548174858093, MinVal:0.7065363526344299, gp: 4.817365717489988e-14\n",
      "Train Acc:97.78%, Test Acc:98.00%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.11755872517824173, MinVal:0.8755607604980469, gp: 4.622041410101738e-17\n",
      "Train Acc:97.73%, Test Acc:98.25%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.02957245334982872, MinVal:1.4351838827133179, gp: 1.025224894691813e-26\n",
      "Train Acc:98.00%, Test Acc:97.95%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.13354676961898804, MinVal:1.0319454669952393, gp: 1.359732027870826e-19\n",
      "Train Acc:98.08%, Test Acc:98.10%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.011627581901848316, MinVal:0.9876639246940613, gp: 4.777143813373902e-19\n",
      "Train Acc:98.40%, Test Acc:98.20%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.01946406625211239, MinVal:1.2139073610305786, gp: 5.664709186196217e-23\n",
      "Train Acc:98.58%, Test Acc:98.20%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.012005649507045746, MinVal:0.8788756132125854, gp: 3.685150509416822e-17\n",
      "Train Acc:98.67%, Test Acc:98.45%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.04065803438425064, MinVal:1.5945026874542236, gp: 1.8195190047677492e-29\n",
      "Train Acc:98.64%, Test Acc:98.50%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.021518578752875328, MinVal:1.468631386756897, gp: 2.809074364849736e-27\n",
      "Train Acc:98.73%, Test Acc:98.45%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.007489198353141546, MinVal:0.850145161151886, gp: 1.1629309732570355e-16\n",
      "Train Acc:98.86%, Test Acc:98.35%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.01418889407068491, MinVal:1.4437518119812012, gp: 5.6869283102454065e-27\n",
      "Train Acc:98.87%, Test Acc:98.40%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.038438376039266586, MinVal:1.7830678224563599, gp: 7.228661368079378e-33\n",
      "Train Acc:98.98%, Test Acc:97.65%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.012455222196877003, MinVal:1.5002753734588623, gp: 5.910661665954927e-28\n",
      "Train Acc:98.91%, Test Acc:98.40%\n",
      "\n",
      "Class: 7 -> Train Acc 98.97500000000001 ; Test Acc 98.5 \n",
      "\n",
      "8\n",
      "Epoch: 0:240,  Loss:0.23314523696899414, MinVal:0.3032788038253784, gp: 3.617627157836978e-07\n",
      "Train Acc:79.67%, Test Acc:96.50%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.10308411717414856, MinVal:0.4413508474826813, gp: 2.4019337629965776e-09\n",
      "Train Acc:96.89%, Test Acc:97.25%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.04119725897908211, MinVal:0.18412049114704132, gp: 3.763002678169869e-05\n",
      "Train Acc:97.32%, Test Acc:97.25%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.017173239961266518, MinVal:0.5971881151199341, gp: 3.953918355920072e-12\n",
      "Train Acc:97.77%, Test Acc:97.40%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.030962908640503883, MinVal:0.5712206959724426, gp: 8.162453352111854e-12\n",
      "Train Acc:98.02%, Test Acc:96.10%\n",
      "\n",
      "Epoch: 5:1440,  Loss:0.01325161475688219, MinVal:0.49787819385528564, gp: 2.4662860642621354e-10\n",
      "Train Acc:98.15%, Test Acc:97.95%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.010764048434793949, MinVal:0.6455259919166565, gp: 4.659854316649281e-13\n",
      "Train Acc:98.28%, Test Acc:97.35%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.13642984628677368, MinVal:0.6644701361656189, gp: 2.1323749515266e-13\n",
      "Train Acc:98.43%, Test Acc:98.15%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.11553727090358734, MinVal:0.8302807211875916, gp: 3.7816370912297436e-16\n",
      "Train Acc:98.61%, Test Acc:98.25%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.006738282274454832, MinVal:1.308449387550354, gp: 1.5580322366781598e-24\n",
      "Train Acc:98.80%, Test Acc:98.05%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.007067051716148853, MinVal:0.7205871343612671, gp: 2.0711459890881967e-14\n",
      "Train Acc:98.66%, Test Acc:98.35%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.03106280416250229, MinVal:0.902204692363739, gp: 2.6088642899573277e-17\n",
      "Train Acc:99.04%, Test Acc:98.25%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.026812737807631493, MinVal:0.6630129814147949, gp: 2.0718923267586814e-13\n",
      "Train Acc:99.11%, Test Acc:98.65%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.03867286816239357, MinVal:0.793328046798706, gp: 1.1286743668316706e-15\n",
      "Train Acc:99.17%, Test Acc:98.40%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.011449095793068409, MinVal:0.7967891097068787, gp: 9.82775694524196e-16\n",
      "Train Acc:99.23%, Test Acc:98.60%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.006521823816001415, MinVal:1.1475361585617065, gp: 7.93134739035333e-22\n",
      "Train Acc:99.31%, Test Acc:98.55%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.14627757668495178, MinVal:1.2432788610458374, gp: 1.7224893039192955e-23\n",
      "Train Acc:99.40%, Test Acc:98.40%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.023219620808959007, MinVal:1.135633111000061, gp: 1.2767962904830381e-21\n",
      "Train Acc:99.21%, Test Acc:98.55%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.021310176700353622, MinVal:0.967664897441864, gp: 1.0574844454951948e-18\n",
      "Train Acc:99.44%, Test Acc:98.70%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.03056797944009304, MinVal:1.4602938890457153, gp: 2.9253997631187472e-27\n",
      "Train Acc:99.54%, Test Acc:98.65%\n",
      "\n",
      "Class: 8 -> Train Acc 99.54166666666666 ; Test Acc 98.7 \n",
      "\n",
      "9\n",
      "Epoch: 0:240,  Loss:0.24286678433418274, MinVal:0.6636513471603394, gp: 2.301045096651799e-13\n",
      "Train Acc:90.08%, Test Acc:96.40%\n",
      "\n",
      "Epoch: 1:480,  Loss:0.07581214606761932, MinVal:0.4413968026638031, gp: 2.241134833269598e-09\n",
      "Train Acc:97.09%, Test Acc:97.45%\n",
      "\n",
      "Epoch: 2:720,  Loss:0.02741255983710289, MinVal:0.5247623920440674, gp: 8.468428042140985e-11\n",
      "Train Acc:97.42%, Test Acc:97.45%\n",
      "\n",
      "Epoch: 3:960,  Loss:0.02411556988954544, MinVal:0.5758180022239685, gp: 7.34369856592032e-12\n",
      "Train Acc:97.45%, Test Acc:97.25%\n",
      "\n",
      "Epoch: 4:1200,  Loss:0.01621500588953495, MinVal:0.7339338660240173, gp: 1.3244506165898621e-14\n",
      "Train Acc:97.64%, Test Acc:97.80%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5:1440,  Loss:0.06201915442943573, MinVal:0.7823418974876404, gp: 2.1398210064076757e-15\n",
      "Train Acc:97.97%, Test Acc:97.90%\n",
      "\n",
      "Epoch: 6:1680,  Loss:0.03540915623307228, MinVal:0.5673704743385315, gp: 9.505384326868871e-12\n",
      "Train Acc:97.92%, Test Acc:98.20%\n",
      "\n",
      "Epoch: 7:1920,  Loss:0.02967522107064724, MinVal:0.7082733511924744, gp: 6.998076434745065e-14\n",
      "Train Acc:98.08%, Test Acc:98.25%\n",
      "\n",
      "Epoch: 8:2160,  Loss:0.0744667649269104, MinVal:0.7415023446083069, gp: 1.3575993356839539e-14\n",
      "Train Acc:98.32%, Test Acc:98.25%\n",
      "\n",
      "Epoch: 9:2400,  Loss:0.06437697261571884, MinVal:0.663670003414154, gp: 2.2499059906810892e-13\n",
      "Train Acc:98.26%, Test Acc:97.40%\n",
      "\n",
      "Epoch: 10:2640,  Loss:0.0743129551410675, MinVal:0.688618540763855, gp: 7.439707893795375e-14\n",
      "Train Acc:98.28%, Test Acc:98.20%\n",
      "\n",
      "Epoch: 11:2880,  Loss:0.1895832121372223, MinVal:0.8117418885231018, gp: 7.78856641758341e-16\n",
      "Train Acc:98.53%, Test Acc:98.30%\n",
      "\n",
      "Epoch: 12:3120,  Loss:0.041259121149778366, MinVal:0.657058835029602, gp: 4.185571431548213e-13\n",
      "Train Acc:98.51%, Test Acc:98.60%\n",
      "\n",
      "Epoch: 13:3360,  Loss:0.05750872194766998, MinVal:0.7958407402038574, gp: 1.6535650141356363e-15\n",
      "Train Acc:98.55%, Test Acc:98.30%\n",
      "\n",
      "Epoch: 14:3600,  Loss:0.00633858609944582, MinVal:0.9956244230270386, gp: 6.163145417288793e-19\n",
      "Train Acc:98.72%, Test Acc:98.35%\n",
      "\n",
      "Epoch: 15:3840,  Loss:0.03867681697010994, MinVal:0.885249137878418, gp: 3.611500332908571e-17\n",
      "Train Acc:98.71%, Test Acc:98.20%\n",
      "\n",
      "Epoch: 16:4080,  Loss:0.007461038883775473, MinVal:0.7641446590423584, gp: 3.6273114469487135e-15\n",
      "Train Acc:98.72%, Test Acc:98.55%\n",
      "\n",
      "Epoch: 17:4320,  Loss:0.01800442300736904, MinVal:0.8601715564727783, gp: 1.505758558933149e-16\n",
      "Train Acc:98.93%, Test Acc:97.70%\n",
      "\n",
      "Epoch: 18:4560,  Loss:0.0349474772810936, MinVal:1.4977790117263794, gp: 6.86633940072147e-28\n",
      "Train Acc:98.94%, Test Acc:98.20%\n",
      "\n",
      "Epoch: 19:4800,  Loss:0.013681240379810333, MinVal:0.8663681745529175, gp: 6.078491070976111e-17\n",
      "Train Acc:98.89%, Test Acc:98.30%\n",
      "\n",
      "Class: 9 -> Train Acc 98.94166666666668 ; Test Acc 98.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stat_per_class = []\n",
    "net_list = []\n",
    "for class_idx in range(10):\n",
    "    print(class_idx)\n",
    "    train_dataset = MNIST_OneClass_Balanced(train_data, train_label, class_idx)\n",
    "    test_dataset = MNIST_OneClass_Balanced(test_data, test_label, class_idx)\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    torch.manual_seed(network_seed)\n",
    "    lips_net = UnivariateCNN([1, 32, 32], actf)\n",
    "    Net = BasicInvexNet(784, lips_net, lambda_)\n",
    "    optimizer = torch.optim.Adam(Net.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    index = 0\n",
    "#     for epoch in range(5):\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for xx, yy in train_loader:\n",
    "            index += 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if use_check and epoch%check_every == 0:\n",
    "                rand_inp = torch.rand(check_size, 784)*m_+s_\n",
    "                Net(rand_inp)\n",
    "                Net.compute_penalty_and_clipper()\n",
    "                Net.gp.backward(retain_graph=True)\n",
    "            \n",
    "            if use_mixup:\n",
    "                rand_indx = np.random.permutation(len(xx))\n",
    "                rand_lambda = 1-torch.rand(len(xx), 1)*0.1\n",
    "                x_mix = rand_lambda*xx+(1-rand_lambda)*xx[rand_indx]\n",
    "                y_mix = rand_lambda*yy+(1-rand_lambda)*yy[rand_indx]\n",
    "            else:\n",
    "                x_mix = xx\n",
    "                y_mix = yy\n",
    "\n",
    "            yout = sigmoid(Net(x_mix))   \n",
    "            Net.compute_penalty_and_clipper()\n",
    "            loss = criterion(yout, y_mix) + Net.gp\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            outputs = (yout.data.cpu().numpy() > 0.5).astype(float)\n",
    "            preds = (yy.data.cpu().numpy() > 0.5).astype(float)\n",
    "            correct = (outputs == preds).astype(float).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             if index%200 == 0:\n",
    "        train_accs.append(float(train_acc)/train_count*100)\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "\n",
    "        min_val, gp = float(Net.cond.min()) , float(Net.gp)\n",
    "        print(f'Epoch: {epoch}:{index},  Loss:{float(loss)}, MinVal:{min_val}, gp: {gp}')\n",
    "        test_count = 0\n",
    "        test_acc = 0\n",
    "        for xx, yy in test_loader:\n",
    "#                     with torch.no_grad():\n",
    "            yout = sigmoid(Net(xx))\n",
    "            outputs = (yout.data.cpu().numpy() > 0.5).astype(float)\n",
    "            correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "            test_acc += correct\n",
    "            test_count += len(xx)\n",
    "        test_accs.append(float(test_acc)/test_count*100)\n",
    "        print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "        print()\n",
    "                \n",
    "    ### after each class index is finished training\n",
    "    stat_per_class.append(\n",
    "    f'Class: {class_idx} -> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}'\n",
    "    )\n",
    "    print(stat_per_class[-1], '\\n')\n",
    "    net_list.append(Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 -> Train Acc 96.30833333333332 ; Test Acc 95.25\n",
      "Class: 1 -> Train Acc 99.45833333333334 ; Test Acc 99.1\n",
      "Class: 2 -> Train Acc 95.14166666666667 ; Test Acc 93.75\n",
      "Class: 3 -> Train Acc 97.36666666666667 ; Test Acc 95.6\n",
      "Class: 4 -> Train Acc 95.30833333333332 ; Test Acc 93.65\n",
      "Class: 5 -> Train Acc 99.25833333333334 ; Test Acc 99.15\n",
      "Class: 6 -> Train Acc 92.63333333333334 ; Test Acc 88.0\n",
      "Class: 7 -> Train Acc 98.97500000000001 ; Test Acc 98.5\n",
      "Class: 8 -> Train Acc 99.54166666666666 ; Test Acc 98.7\n",
      "Class: 9 -> Train Acc 98.94166666666668 ; Test Acc 98.6\n",
      "Total Accuracy (Argmax) is : 0.878000020980835\n"
     ]
    }
   ],
   "source": [
    "## find the classification error from classification per class network.\n",
    "acc_test = 0\n",
    "count_test = 0\n",
    "with torch.no_grad():\n",
    "    for index in range(len(test_label) // batch_size):\n",
    "        xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "        yy = test_label[index * batch_size:(index + 1) * batch_size]\n",
    "        yout = []\n",
    "        for net in net_list:\n",
    "            yout.append(sigmoid(net(xx)).reshape(-1))\n",
    "        yout = torch.stack(yout, dim=1).argmax(dim=1)\n",
    "        acc = (yout == yy).type(torch.float32).sum()\n",
    "        count_test += len(xx)\n",
    "        acc_test += acc\n",
    "        \n",
    "for s in stat_per_class:\n",
    "    print(s)\n",
    "print(f\"Total Accuracy (Argmax) is : {float(acc_test/count_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## find the classification acc for confident examples \n",
    "# acc_test = 0\n",
    "# count_test = 0\n",
    "# certain_percent = 0\n",
    "# with torch.no_grad():\n",
    "#     for index in range(len(test_label) // batch_size):\n",
    "#         xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "#         yy = test_label[index * batch_size:(index + 1) * batch_size]\n",
    "#         yout = []\n",
    "#         for net in net_list:\n",
    "#             yout.append(sigmoid(net(xx)).reshape(-1))\n",
    "#         yout_ = torch.stack(yout, dim=1)\n",
    "# #         print(yout_)\n",
    "# #         print(yy)\n",
    "# #         break\n",
    "#         yout_, yout = yout_.max(dim=1)\n",
    "#         mask = yout_<0.5\n",
    "#         certain_percent += (mask).type(torch.float32).sum()\n",
    "#         acc = (yout == yy).type(torch.float32)[~mask].sum()\n",
    "#         count_test += len(xx)\n",
    "#         acc_test += acc\n",
    "        \n",
    "# print(f\"UNConfident {float(certain_percent/count_test)} (Argmax) is : {float(acc_test/(count_test-certain_percent))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [03:34<00:00, 93.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 -> Correct 99.9739% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [03:32<00:00, 93.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 1 -> Correct 99.9064% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [04:04<00:00, 81.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 2 -> Correct 98.4567% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [06:17<00:00, 52.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 3 -> Correct 99.9158% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [06:24<00:00, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 4 -> Correct 99.8463% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [06:22<00:00, 52.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 5 -> Correct 100.0000% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [06:12<00:00, 53.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 6 -> Correct 99.8244% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [06:27<00:00, 51.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 7 -> Correct 100.0000% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [05:12<00:00, 64.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 8 -> Correct 99.9998% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20000/20000 [03:34<00:00, 93.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 9 -> Correct 99.9999% on 1070000 input points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the constraint on large number of points, including training and test data.\n",
    "\n",
    "for class_idx in range(10):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    net = net_list[class_idx]\n",
    "    for index in range(len(test_label) // batch_size):\n",
    "        xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "        net(xx)\n",
    "        net.compute_penalty_and_clipper()\n",
    "        correct += (net.cond>0).type(torch.float32).sum()\n",
    "        count += len(xx)\n",
    "        \n",
    "    for index in range(len(train_label) // batch_size):\n",
    "        xx = train_data[index * batch_size:(index + 1) * batch_size]\n",
    "        net(xx)\n",
    "        net.compute_penalty_and_clipper()\n",
    "        correct += (net.cond>0).type(torch.float32).sum()\n",
    "        count += len(xx)\n",
    "\n",
    "    for i in tqdm(range(20000)):\n",
    "        xx = torch.rand(batch_size, 784)\n",
    "        net(xx)\n",
    "        net.compute_penalty_and_clipper()\n",
    "        correct += (net.cond>0).type(torch.float32).sum()\n",
    "        count += len(xx)\n",
    "\n",
    "    print(f\"Class: {class_idx} -> Correct {correct/count*100:.4f}% on {count} input points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 -> Correct 100.0000% on 70000 input points\n",
      "Class: 1 -> Correct 100.0000% on 70000 input points\n",
      "Class: 2 -> Correct 100.0000% on 70000 input points\n",
      "Class: 3 -> Correct 99.9971% on 70000 input points\n",
      "Class: 4 -> Correct 99.9929% on 70000 input points\n",
      "Class: 5 -> Correct 100.0000% on 70000 input points\n",
      "Class: 6 -> Correct 99.9986% on 70000 input points\n",
      "Class: 7 -> Correct 100.0000% on 70000 input points\n",
      "Class: 8 -> Correct 99.9971% on 70000 input points\n",
      "Class: 9 -> Correct 99.9986% on 70000 input points\n"
     ]
    }
   ],
   "source": [
    "## only on training and testing data\n",
    "for class_idx in range(10):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    net = net_list[class_idx]\n",
    "    for index in range(len(test_label) // batch_size):\n",
    "        xx = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "        net(xx)\n",
    "        net.compute_penalty_and_clipper()\n",
    "        correct += (net.cond>0).type(torch.float32).sum()\n",
    "        count += len(xx)\n",
    "        \n",
    "    for index in range(len(train_label) // batch_size):\n",
    "        xx = train_data[index * batch_size:(index + 1) * batch_size]\n",
    "        net(xx)\n",
    "        net.compute_penalty_and_clipper()\n",
    "        correct += (net.cond>0).type(torch.float32).sum()\n",
    "        count += len(xx)\n",
    "\n",
    "    print(f\"Class: {class_idx} -> Correct {correct/count*100:.4f}% on {count} input points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.compute_penalty_and_clipper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
